{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Sagemaker Modeling Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *I used this [post](https://blog.betomorrow.com/keras-in-the-cloud-with-amazon-sagemaker-67cf11fb536) from Paul Breton and the corresponding GitHub [repo](https://github.com/Pravez/KerasSageMaker) for guidance on utilizing Keras with Sagemaker.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries to Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import tensorflow\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from keras.layers import Dropout\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from tensorflow.python.keras.preprocessing.image import load_img\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sagemaker Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Sagemaker session and role ARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have to define our S3 bucket name and pathway to training and validation sets, and set some hyperparameters for the model to use when calling the Keras model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"sagemaker-all-cnn\"\n",
    "key = \"Data\"\n",
    "key_output = \"output\"                   # Path from the bucket's root to the dataset\n",
    "train_instance_type='ml.m4.xlarge'      # The type of EC2 instance which will be used for training\n",
    "deploy_instance_type='ml.m4.xlarge'     # The type of EC2 instance which will be used for deployment\n",
    "hyperparameters={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"decay\": 0.0001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify locations for our data within the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_path = \"s3://{}/{}/training/\".format(bucket, key)\n",
    "validation_input_path = \"s3://{}/{}/validation/\".format(bucket, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the actual training job. We specify additional hyperparameters, such as epochs and evaluation steps, in addition to loading the model architecture from a separate Python script (which can be found in the Model_Scripts directory in this repo) where we use the traditional Keras framework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(\n",
    "  entry_point=os.path.join(os.path.dirname('__file__'), \"Model_Scripts/Model_2x2x1x1C1D.py\"),\n",
    "  role=role,\n",
    "  framework_version=\"1.12.0\",               # TensorFlow's version\n",
    "  hyperparameters=hyperparameters,\n",
    "  training_steps=100,\n",
    "  evaluation_steps=30,\n",
    "  train_instance_count=1,                   # \"The number of GPUs instances to use\"\n",
    "  train_instance_type=train_instance_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the .fit() method to begin the training job. Progress is tracked here, but the actual computation is done on a separate Training Jobs instance in AWS with a ml.m4.xlarge instance (a hyperparameter we can alter if the respective resource quota is satisfied). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the model architecture, training takes from 10 mins to 1 hour with 4000 images and 100-150 epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running this, you should see prompts in the following structure displayed over about 3 minutes. Most errors will be thrown in the last \"Training in progress.\" step, so wait until that step is complete before navigating away.\n",
    "\n",
    "> \"Training ... \n",
    "\n",
    ">2020-09-22 16:09:18 Starting - Starting the training job...\n",
    "\n",
    ">2020-09-22 16:09:20 Starting - Launching requested ML instances......\n",
    "\n",
    ">2020-09-22 16:10:30 Starting - Preparing the instances for training...\n",
    "\n",
    ">2020-09-22 16:11:19 Downloading - Downloading input data.........\n",
    "\n",
    ">2020-09-22 16:12:40 Training - Training image download completed. Training in progress.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training ...\")\n",
    "estimator.fit({'training': train_input_path, 'eval': validation_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model checkpoints and the final trained model will be uploaded to a new S3 bucket created by Sagemaker to store all models from this notebook instance. Billable seconds will also be calculated for the length of training and add to relevant totals in the AWS Billing dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Sagemaker Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use .deploy() method on the trained estimator. This will take about 5 minutes to run and an AWS Endpoint will be created with the parameters defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying ...\n",
      "-------------!"
     ]
    }
   ],
   "source": [
    "print(\"Deploying ...\")\n",
    "predictor = estimator.deploy(initial_instance_count=1, instance_type=deploy_instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides the model ID that is being hosted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor endpoint name : sagemaker-tensorflow-2020-09-29-14-14-32-855\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictor endpoint name : %s\" % predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load not-yet-seen image to be predicted and resize it to match the model input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing endpoint ...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"Testing endpoint ...\")\n",
    "all_image = load_img('./Images/ALL_Testing/419.bmp', target_size=(256, 256))\n",
    "all_image_array = np.array(all_image).reshape((1, 256, 256, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns a JSON object where <'float_val': [1.0, 0.0]> represents the predicted class for the loaded image. The left value is the positive class, leukemia, and the right value is the negative class, normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'outputs': {u'dense_1': {'dtype': 1, 'float_val': [1.0, 0.0], 'tensor_shape': {'dim': [{'size': 1L}, {'size': 2L}]}}}, 'model_spec': {'signature_name': u'serving_default', 'version': {'value': 1601390206L}, 'name': u'generic_model'}}\n"
     ]
    }
   ],
   "source": [
    "print(predictor.predict({'inputs_input': all_image_array}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p27",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
